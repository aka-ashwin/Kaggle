{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#more imports; mine, not there by default on Kaggle Kernel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "mae_scoring = make_scorer(mean_absolute_error)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/train.csv\")\n",
    "y = df['loss']\n",
    "\n",
    "#write losses to csv\n",
    "y.to_csv(\"./transf/loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find columns w nas\n",
    "print(df.columns[pd.isnull(df).any()].tolist())\n",
    "#none\n",
    "\n",
    "#get source (non-id, non-loss) column names\n",
    "src = list(df.columns)\n",
    "src.remove('id')\n",
    "src.remove('loss')\n",
    "\n",
    "len(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dummy out categorical variables so we can use numerical dimensionality reduction + other methods\n",
    "ind = 1\n",
    "for var in src:\n",
    "    if var[:3] == \"cat\":\n",
    "#         print(var)\n",
    "        dums = pd.get_dummies(df[var], prefix = \"dum\" + str(ind), drop_first = True )\n",
    "        df = pd.concat([df,dums], axis=1)\n",
    "        ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write dataframe with all necessary columns to csv\n",
    "# df.to_csv(\"./transf/train_dummies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write dataframe w dummies and conts only to csv\n",
    "\n",
    "#inputs = [col for col in df.columns if(col[:3] == 'dum' or col[:4] == 'cont')]\n",
    "#df = df[inputs]\n",
    "\n",
    "# df.to_csv(\"./transf/train_dummies_nocats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalize continuous variables\n",
    "cols_to_norm = [col for col in df.columns if col[:4] == \"cont\"]\n",
    "\n",
    "df[cols_to_norm] = df[cols_to_norm].apply(lambda x: (x - x.mean()) / np.sqrt(x.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cont1        0.990890\n",
       "cont2        0.996037\n",
       "cont3        1.009693\n",
       "cont4        0.998074\n",
       "cont5        0.973569\n",
       "cont6        0.989288\n",
       "cont7        1.023593\n",
       "cont8        1.006121\n",
       "cont9        0.997499\n",
       "cont10       0.987978\n",
       "cont11       1.012633\n",
       "cont12       1.010119\n",
       "cont13       0.996892\n",
       "cont14       1.004844\n",
       "dum1_B       0.185350\n",
       "dum2_B       0.245068\n",
       "dum3_B       0.051178\n",
       "dum4_B       0.216642\n",
       "dum5_B       0.228502\n",
       "dum6_B       0.213119\n",
       "dum7_B       0.024377\n",
       "dum8_B       0.055701\n",
       "dum9_B       0.239518\n",
       "dum10_B      0.126249\n",
       "dum11_B      0.094142\n",
       "dum12_B      0.126741\n",
       "dum13_B      0.091446\n",
       "dum14_B      0.011760\n",
       "dum15_B      0.000100\n",
       "dum16_B      0.031073\n",
       "               ...   \n",
       "dum116_ME    0.001598\n",
       "dum116_MF    0.000000\n",
       "dum116_MG    0.001797\n",
       "dum116_MH    0.000000\n",
       "dum116_MI    0.000600\n",
       "dum116_MJ    0.003289\n",
       "dum116_MK    0.000200\n",
       "dum116_ML    0.000100\n",
       "dum116_MM    0.000000\n",
       "dum116_MN    0.000100\n",
       "dum116_MO    0.000300\n",
       "dum116_MP    0.000400\n",
       "dum116_MQ    0.000000\n",
       "dum116_MR    0.000100\n",
       "dum116_MS    0.000000\n",
       "dum116_MT    0.000000\n",
       "dum116_MU    0.000200\n",
       "dum116_MV    0.000000\n",
       "dum116_MW    0.000000\n",
       "dum116_O     0.000000\n",
       "dum116_P     0.000000\n",
       "dum116_Q     0.000000\n",
       "dum116_R     0.000100\n",
       "dum116_S     0.000000\n",
       "dum116_T     0.000000\n",
       "dum116_U     0.000100\n",
       "dum116_V     0.000100\n",
       "dum116_W     0.000000\n",
       "dum116_X     0.000000\n",
       "dum116_Y     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write df with dummies and normalized conts to csv\n",
    "#df.to_csv(\"./transf/train_inputs.csv\")\n",
    "\n",
    "#read df w dummies and normalized conts - using a fraction of rows so I can try things out without being too slow\n",
    "df = pd.read_csv(\"./transf/train_inputs.csv\", nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = np.asarray(df[inputs])\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit_transform(df)\n",
    "\n",
    "exp = pca.explained_variance_ratio_\n",
    "cumvar = np.cumsum(exp)\n",
    "plt.clf()\n",
    "plt.plot(range(len(cumvar)), cumvar)\n",
    "plt.show()\n",
    "#99% of variance is included in first 353 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_take = 353\n",
    "X = df[:,:n_take]\n",
    "\n",
    "yold = list(y)\n",
    "y = yold[:len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1860.00926815,  1856.41639364,  1886.26390639])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing out some regressors, getting a sense of what error they have\n",
    "#even after trimming down features with PCA, fitting a regressor takes a while: 10+min for an mae-criterion decision tree\n",
    "#seems like a good potential place for a booster - might be many subpatterns to fit local greedy algs to\n",
    "\n",
    "#tried linear regression just to see - it performs abysmally (error ~ 10^6)\n",
    "#Decision Tree opt for mse gets ~1700\n",
    "#Random Forest opt for mse gets ~1400\n",
    "\n",
    "reg = DecisionTreeRegressor(criterion = 'mae')\n",
    "# reg = RandomForestRegressor(n_estimators = 3, criterion = 'mae')\n",
    "\n",
    "score = cross_val_score(reg,X,y,scoring = mae_scoring)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = kNeighborsRegressor(weight = 'uniform')\n",
    "tovary = \"n_neighbors\"\n",
    "vary_range = range(2,6,2)\n",
    "\n",
    "train_scores, valid_scores = learning_curve(reg, X,y, tovary, vary_range)\n",
    "\n",
    "trainavg = np.mean(train_scores, axis=1)\n",
    "validavg = np.mean(valid_scores, axis=1)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(vary_range, trainavg, color = 'k', marker = '.')\n",
    "plt.plot(vary_range, validavg, color = 'g', marker = '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3502beef80e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./input/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#write losses to csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./transf/loss.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dft = pd.read_csv(\"./input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-097248590cb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#find columns w nas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#none\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#get source (non-id, non-loss) column names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dft' is not defined"
     ]
    }
   ],
   "source": [
    "#find columns w nas\n",
    "print(dft.columns[pd.isnull(dft).any()].tolist())\n",
    "#none\n",
    "\n",
    "#get source (non-id, non-loss) column names\n",
    "src = list(dft.columns)\n",
    "src.remove('id')\n",
    "src.remove('loss')\n",
    "\n",
    "len(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dummy out categorical variables so we can use numerical dimensionality reduction + other methods\n",
    "ind = 1\n",
    "for var in src:\n",
    "    if var[:3] == \"cat\":\n",
    "#         print(var)\n",
    "        dums = pd.get_dummies(dft[var], prefix = \"dum\" + str(ind), drop_first = True )\n",
    "        dft = pd.concat([dft,dums], axis=1)\n",
    "        ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write dataframe with all columns to csv\n",
    "dft.to_csv(\"./transf/test_dummies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write dataframe w dummies and conts only to csv\n",
    "\n",
    "dft = dft[inputs]\n",
    "\n",
    "dft.to_csv(\"./transf/test_dummies_nocats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalize continuous variables\n",
    "cols_to_norm = [col for col in dft.columns if col[:4] == \"cont\"]\n",
    "\n",
    "dft[cols_to_norm] = dft[cols_to_norm].apply(lambda x: (x - x.mean()) / np.sqrt(x.var()))\n",
    "\n",
    "#write df with dummies and normalized conts to csv\n",
    "dft.to_csv(\"./transf/test_inputs.csv\")\n",
    "\n",
    "##read df w dummies and normalized conts - using a fraction of rows so I can try things out without being too slow\n",
    "#dft = pd.read_csv(\"./transf/train_inputs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
